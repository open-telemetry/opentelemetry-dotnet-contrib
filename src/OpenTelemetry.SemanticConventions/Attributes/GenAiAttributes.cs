// Copyright The OpenTelemetry Authors
// SPDX-License-Identifier: Apache-2.0

// <auto-generated>This file has been auto generated from 'src\OpenTelemetry.SemanticConventions\scripts\templates\registry\SemanticConventionsAttributes.cs.j2' </auto-generated>

#nullable enable

#pragma warning disable CS1570 // XML comment has badly formed XML

namespace OpenTelemetry.SemanticConventions;

/// <summary>
/// Constants for semantic attribute names outlined by the OpenTelemetry specifications.
/// </summary>
public static class GenAiAttributes
{
    /// <summary>
    /// The full response received from the LLM
    /// </summary>
    /// <remarks>
    /// It's RECOMMENDED to format completions as JSON string matching <a href="https://platform.openai.com/docs/guides/text-generation">OpenAI messages format</a>
    /// </remarks>
    public const string AttributeGenAiCompletion = "gen_ai.completion";

    /// <summary>
    /// The full prompt sent to an LLM
    /// </summary>
    /// <remarks>
    /// It's RECOMMENDED to format prompts as JSON string matching <a href="https://platform.openai.com/docs/guides/text-generation">OpenAI messages format</a>
    /// </remarks>
    public const string AttributeGenAiPrompt = "gen_ai.prompt";

    /// <summary>
    /// The maximum number of tokens the LLM generates for a request
    /// </summary>
    public const string AttributeGenAiRequestMaxTokens = "gen_ai.request.max_tokens";

    /// <summary>
    /// The name of the LLM a request is being made to
    /// </summary>
    public const string AttributeGenAiRequestModel = "gen_ai.request.model";

    /// <summary>
    /// The temperature setting for the LLM request
    /// </summary>
    public const string AttributeGenAiRequestTemperature = "gen_ai.request.temperature";

    /// <summary>
    /// The top_p sampling setting for the LLM request
    /// </summary>
    public const string AttributeGenAiRequestTopP = "gen_ai.request.top_p";

    /// <summary>
    /// Array of reasons the model stopped generating tokens, corresponding to each generation received
    /// </summary>
    public const string AttributeGenAiResponseFinishReasons = "gen_ai.response.finish_reasons";

    /// <summary>
    /// The unique identifier for the completion
    /// </summary>
    public const string AttributeGenAiResponseId = "gen_ai.response.id";

    /// <summary>
    /// The name of the LLM a response was generated from
    /// </summary>
    public const string AttributeGenAiResponseModel = "gen_ai.response.model";

    /// <summary>
    /// The Generative AI product as identified by the client instrumentation
    /// </summary>
    /// <remarks>
    /// The actual GenAI product may differ from the one identified by the client. For example, when using OpenAI client libraries to communicate with Mistral, the <c>gen_ai.system</c> is set to <c>openai</c> based on the instrumentation's best knowledge
    /// </remarks>
    public const string AttributeGenAiSystem = "gen_ai.system";

    /// <summary>
    /// The number of tokens used in the LLM response (completion)
    /// </summary>
    public const string AttributeGenAiUsageCompletionTokens = "gen_ai.usage.completion_tokens";

    /// <summary>
    /// The number of tokens used in the LLM prompt
    /// </summary>
    public const string AttributeGenAiUsagePromptTokens = "gen_ai.usage.prompt_tokens";

    /// <summary>
    /// The Generative AI product as identified by the client instrumentation
    /// </summary>
    public static class GenAiSystemValues
    {
        /// <summary>
        /// OpenAI
        /// </summary>
        public const string Openai = "openai";
    }
}
